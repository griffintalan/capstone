{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, stem_data, remove_stopwords):\n",
    "    processed = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for file in data:\n",
    "        \n",
    "        # lowercasing all text\n",
    "        \n",
    "        file = str(file).lower()\n",
    "        \n",
    "        \n",
    "        # removing non-alpha characters\n",
    "        file = re.sub('[^a-zA-Z]', ' ', file)\n",
    "        \n",
    "        # tokenizing articles\n",
    "        tokenized = word_tokenize(file)\n",
    "        \n",
    "        # removing stop words from tokens\n",
    "        stop_removed_tokens = []\n",
    "        if remove_stopwords:\n",
    "            for word in tokenized:\n",
    "                if word not in stop_words:\n",
    "                    stop_removed_tokens.append(word)\n",
    "        else:\n",
    "            stop_removed_tokens = tokenized\n",
    "        if stem_data:\n",
    "            stemmed = []\n",
    "            for token in stop_removed_tokens:\n",
    "                stemmed.append(stemmer.stem(token))\n",
    "            processed.append(stemmed)\n",
    "        else:\n",
    "            processed.append(stop_removed_tokens)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu = pd.read_csv('../data/prediction_data/formatted_pred_data/sotu_formatted.csv')\n",
    "sotu['processed_no_stem'] = preprocess(sotu['Text'], stem_data = False, remove_stopwords = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaug = pd.read_csv('../data/prediction_data/formatted_pred_data/inaugural_formatted.csv')\n",
    "inaug['processed_no_stem'] = preprocess(inaug['text'], stem_data = False, remove_stopwords = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "court = pd.read_csv('../data/prediction_data/formatted_pred_data/court_formatted.csv')\n",
    "court['processed_no_stem'] = preprocess(court['text'], stem_data = False, remove_stopwords = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doj = pd.read_csv('../data/prediction_data/formatted_pred_data/doj_formatted.csv')\n",
    "doj['processed_no_stem'] = preprocess(doj['contents'], stem_data = False, remove_stopwords = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjectivity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting model parameters\n",
    "\n",
    "vocab_size_subj_obj = 5000\n",
    "embedding_dimension_subj_obj = 8\n",
    "max_length_subj_obj = 200\n",
    "trunc_type_subj_obj = 'post'\n",
    "padding_type_subj_obj = 'post'\n",
    "oov_tok_subj_obj = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "# importing model and pickled tokenizer \n",
    "\n",
    "model_subj_obj = load_model('../data/train_data/subjectivity/subj_obj_model.h5')\n",
    "\n",
    "with open('../data/train_data/subjectivity/tokenizer_subj_obj.pickle', 'rb') as handle:\n",
    "    tokenizer_subj_obj = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***State of the Union Speeches***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializing and padding sotu speeches with tokenizer\n",
    "\n",
    "sotu_sequences_subj_obj = tokenizer_subj_obj.texts_to_sequences(sotu['processed_no_stem'])\n",
    "sotu_padded_subj_obj = pad_sequences(sotu_sequences_subj_obj, \n",
    "                                     maxlen=max_length_subj_obj, \n",
    "                                     padding=padding_type_subj_obj, \n",
    "                                     truncating=trunc_type_subj_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sotu predictions and appending to original df\n",
    "\n",
    "sotu_subj_obj_preds = model_subj_obj.predict(sotu_padded_subj_obj)\n",
    "sotu['is_subjective_preds'] = sotu_subj_obj_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Department of Justice Statements***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializing and padding DOJ statements with tokenizer\n",
    "\n",
    "doj_sequences_subj_obj = tokenizer_subj_obj.texts_to_sequences(doj['processed_no_stem'])\n",
    "doj_padded_subj_obj = pad_sequences(doj_sequences_subj_obj, \n",
    "                                     maxlen=max_length_subj_obj, \n",
    "                                     padding=padding_type_subj_obj, \n",
    "                                     truncating=trunc_type_subj_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making doj predictions and appending to original df\n",
    "\n",
    "doj_subj_obj_preds = model_subj_obj.predict(doj_padded_subj_obj)\n",
    "doj['is_subjective_preds'] = doj_subj_obj_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Supreme Court Decisions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializing and padding supreme court decisions with tokenizer\n",
    "\n",
    "court_sequences_subj_obj = tokenizer_subj_obj.texts_to_sequences(court['processed_no_stem'])\n",
    "court_padded_subj_obj = pad_sequences(court_sequences_subj_obj, \n",
    "                                     maxlen=max_length_subj_obj, \n",
    "                                     padding=padding_type_subj_obj, \n",
    "                                     truncating=trunc_type_subj_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making supreme court predictions and appending to original df\n",
    "\n",
    "court_subj_obj_preds = model_subj_obj.predict(court_padded_subj_obj)\n",
    "court['is_subjective_preds'] = court_subj_obj_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Inaugural Addresses***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializing and padding inaugural addresses with tokenizer\n",
    "\n",
    "inaug_sequences_subj_obj = tokenizer_subj_obj.texts_to_sequences(inaug['processed_no_stem'])\n",
    "inaug_padded_subj_obj = pad_sequences(inaug_sequences_subj_obj, \n",
    "                                     maxlen=max_length_subj_obj, \n",
    "                                     padding=padding_type_subj_obj, \n",
    "                                     truncating=trunc_type_subj_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making inaugural address predictions and appending to original df\n",
    "\n",
    "inaug_subj_obj_preds = model_subj_obj.predict(inaug_padded_subj_obj)\n",
    "inaug['is_subjective_preds'] = inaug_subj_obj_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_sent = 5000\n",
    "embedding_dimension_sent = 10\n",
    "max_length_sent = 1000\n",
    "trunc_type_sent = 'post'\n",
    "padding_type_sent = 'post'\n",
    "oov_tok_sent = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "model_sent = load_model('../data/train_data/sentiment/sent_model.h5')\n",
    "\n",
    "with open('../data/train_data/sentiment/tokenizer_sent.pickle', 'rb') as handle:\n",
    "    tokenizer_sent = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***State of the Union Addresses***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializing and padding sotu speeches with tokenizer\n",
    "\n",
    "sotu_sequences_sent = tokenizer_sent.texts_to_sequences(sotu['processed_no_stem'])\n",
    "sotu_padded_sent = pad_sequences(sotu_sequences_sent, \n",
    "                                     maxlen=max_length_sent, \n",
    "                                     padding=padding_type_sent, \n",
    "                                     truncating=trunc_type_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sotu predictions and appending to original df\n",
    "\n",
    "sotu_sent_preds = model_sent.predict(sotu_padded_sent)\n",
    "sotu['is_positive_preds'] = sotu_sent_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Department of Justice Statements***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializing and padding DOJ statements with tokenizer\n",
    "\n",
    "doj_sequences_sent = tokenizer_sent.texts_to_sequences(doj['processed_no_stem'])\n",
    "doj_padded_sent = pad_sequences(doj_sequences_sent, \n",
    "                                     maxlen=max_length_sent, \n",
    "                                     padding=padding_type_sent, \n",
    "                                     truncating=trunc_type_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making doj predictions and appending to original df\n",
    "\n",
    "doj_sent_preds = model_sent.predict(doj_padded_sent)\n",
    "doj['is_positive_preds'] = doj_sent_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Supreme Court Decisions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializing and padding supreme court decisions with tokenizer\n",
    "\n",
    "court_sequences_sent = tokenizer_sent.texts_to_sequences(court['processed_no_stem'])\n",
    "court_padded_sent = pad_sequences(court_sequences_sent, \n",
    "                                     maxlen=max_length_sent, \n",
    "                                     padding=padding_type_sent, \n",
    "                                     truncating=trunc_type_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making supreme court predictions and appending to original df\n",
    "\n",
    "court_sent_preds = model_sent.predict(court_padded_sent)\n",
    "court['is_positive_preds'] = court_sent_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Inaugural Addresses***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializing and padding inaugural addresses with tokenizer\n",
    "\n",
    "inaug_sequences_sent = tokenizer_sent.texts_to_sequences(inaug['processed_no_stem'])\n",
    "inaug_padded_sent = pad_sequences(inaug_sequences_sent, \n",
    "                                     maxlen=max_length_sent, \n",
    "                                     padding=padding_type_sent, \n",
    "                                     truncating=trunc_type_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making inaugural address predictions and appending to original df\n",
    "\n",
    "inaug_sent_preds = model_sent.predict(inaug_padded_sent)\n",
    "inaug['is_positive_preds'] = inaug_sent_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu.to_csv('../data/prediction_data/predictions/sotu.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaug.to_csv('../data/prediction_data/predictions/inaug.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "court.to_csv('../data/prediction_data/predictions/court.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "doj.to_csv('../data/prediction_data/predictions/doj.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
